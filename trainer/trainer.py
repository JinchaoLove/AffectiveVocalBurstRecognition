import os
import gc
import inspect
import traceback
import threading
from itertools import chain
from functools import partial
from collections import OrderedDict
from pathlib import Path
import numpy as np
import pandas as pd
from tqdm import tqdm
from sklearn.base import BaseEstimator
from sklearn.utils.validation import check_is_fitted
import torch
from torch.utils.data import Dataset, DataLoader
from torch.nn.parallel.distributed import DistributedDataParallel as DDP
from torchinfo import summary
from pytorch_lightning.lite import LightningLite
from pytorch_lightning.lite.wrappers import _LiteModule, _LiteOptimizer
from pytorch_lightning.strategies import ParallelStrategy
from .history import History
from .callbacks import SignalConnector, EpochTimer, PrintLog, LearningRate, PassthroughScoring, EpochScoring
from .utils import seed_everything, to_numpy, to_device, to_tensor, infer_predict_nonlin, BaseDataset, encode_str, decode_str, match


# pylint: disable=too-many-public-methods
class Trainer(LightningLite):
    """
    TODO: Integrate with lightning callbacks and metrics.
    References:
        - [skorch](https://github.com/skorch-dev/skorch/blob/master/skorch/net.py)
        - [LightningLite](https://github.com/Lightning-AI/lightning/blob/master/src/pytorch_lightning/lite/lite.py)
    Description:
        A base class for trainer with loggers and callbacks.
        In addition to the args listed below, it can handle parameters with specific prefixes separately.
        e.g.,
        ```
        trainer = Trainer(..., optimizer=torch.optim.AdamW, optimizer__weight_decay=0.01)  # allows to pass kwargs with `__`
        net.set_params(optimizer__momentum=0.99)  # allows to change parameters later
        net.set_params(callbacks__trn_acc__average='weighted')  # allows pass nested kwargs with `__`
        net.initialize()  # call `initialize` after set parameters (included at the beginning of `fit`)
        ```
        Default special prefixes:
            Dataloaders : `iterator_trn`, `iterator_val`, `iterator_tst`, `iterator` (all)
            Modules : `module`, `criterion`, `optimizer`
            Callbacks : `callbacks`
        Call functions: trainer.run(method, *args, **kwargs)
    Args:
        **LITE : LightningLite kwargs for training environment, e.g., DDP, Mixed precision
        module : torch module (class or instance)
        criterion : torch criterion (class, default=CrossEntropyLoss)
        optimizer : torch optim (class, default=AdamW)
        nonlin : callable, None, or 'auto' (default='auto'). The nonlinearity to be applied to the prediction, e.g., `softmax`.
        lr : float (default=0.01), same as `optimizer__lr`
        max_epochs : int (default=10)
        batch_size : int (default=128)
        accumulate_steps : int (default=1). gradient accumulating for larger batch size
        grad_clip_value : float (default=10)
        dataset : torch Dataset (default=BaseDataset), wrapper if incoming data is not `torch.utils.data.Dataset`
        callbacks : None, "disable", or list of Callback instances (default=None)
        warm_start : bool (default=False). Whether the module should be trained further or not.
        ddp_test: bool (default=False). Whether use distributed sampler for validation and test (output will be all_gathered)
        verbose : int (default=1). How much print output is generated by the Trainer and its callbacks
        seed : int (default=None). Global environment seed.
        init_env: bool (default=False). Flag of whether environment has been setup.
    Logic of `fit`:
        1. setup environment (`LightningLite`), module, criterion, optimizer, logger (`History`), callbacks (`initialize`).
        2. prepare train, validation and test dataloaders (`get_dataloaders`), match to environment (`setup_dataloaders`)
        3. `run_single_epoch` : dataset -> dataloader -> batch -> `step_fn` (`train_step`, `val_step`)
            1) train_step : set_training(True) -> infer/forward, include:
                unpack_data -> module_.forward -> get loss with criterion_ -> backward -> grad accumulating -> grad clip -> optim step
            2) val_step : set_training(False) -> infer/forward with no grad
    Logic of `predict`:
        1. predict_proba : forward_iter (get_iterator -> get outputs from evaluation_step) -> nonlin_
        2. argmax
    Logic of logger and callbacks (`notify`):
        Call method names when : on_train_begin -> on_epoch_begin -> on_batch_begin -> on_batch_end -> on_epoch_end -> on_train_end
    Auxiliary functions:
        - _signal_connector : set `self.should_stop_=True` when get `ctrl + c` signal
        - check_stop : if `self.should_stop_=True`, broadcast and stop every process. `check_stop(True)` will save the last model.
        - model_summary : summarize the given torch model (Layer names, # of parameters, depth=Trainer.verbose)
        - find_unused_params : find_unused_params in distributed environment
        - optim_params : per-parameter options for optimizer
        - get_params/set_params/save_params/load_params : deal with parameters, save or load model/optimizer/history
    """
    LITE = {
        'accelerator': 'auto',
        'strategy': None,
        'devices': '0,',
        'num_nodes': 1,
        'precision': 32,
        'plugins': None,
        'gpus': None,
        'tpu_cores': None
    }
    prefixes_ = ['iterator', 'iterator_trn', 'iterator_val', 'iterator_tst', 'dataset', 'callbacks']

    def __init__(
        self,
        module=None,
        criterion=torch.nn.CrossEntropyLoss,
        optimizer=torch.optim.AdamW,
        nonlin='auto',
        lr=0.01,
        max_epochs=10,
        batch_size=128,
        accumulate_steps=1,
        grad_clip_value=10,
        dataset=BaseDataset,
        callbacks=None,
        log_interval=1,
        warm_start=False,
        ddp_test=False,
        verbose=1,
        seed=None,
        reset_grad=False,
        saved_dir=os.path.abspath(os.path.join(__file__, "../../outputs")),
        **kwargs
    ):
        super().__init__(**(self.LITE | {k: v for k, v in kwargs.items() if k in self.LITE.keys()}))
        self.module = module
        self.criterion = criterion
        self.optimizer = optimizer
        self.nonlin = nonlin
        self.max_epochs = max_epochs
        self.lr = lr
        self.batch_size = batch_size
        self.accumulate_steps = accumulate_steps
        self.grad_clip_value = grad_clip_value
        self.dataset = dataset
        self.callbacks = callbacks
        self.log_interval = log_interval
        self.warm_start = warm_start
        self.ddp_test = ddp_test
        self.verbose = verbose
        self.seed = seed
        self.reset_grad = reset_grad
        self.saved_dir = saved_dir

        self.note = f"\033[1;34m♫{self.local_rank}\033[0m"
        for func in ["fit", "predict"]:
            setattr(self, func, partial(self._run_impl, getattr(self, func)))
        self._signal_connector = SignalConnector(self)

        history = kwargs.pop('history', None)
        initialized = kwargs.pop("initialized_", False)
        vars(self).update(kwargs)  # store kwargs for BaseEstimator
        self.history_ = history
        self.initialized_ = initialized

        # activate environment
        self.signal_handler()
        seed_everything(self.seed)
        self.run()
        self.check_stop()

    def initialize(self):
        """Initialize env, module, criterion, optimizer, logger, and callbacks."""
        self.batch_size_ = self.batch_size // self.num_devices
        self.signal_handler()
        self.initialize_module()
        self.initialize_criterion()
        self.initialize_optimizer()
        self.nonlin_ = infer_predict_nonlin(self) if self.nonlin == 'auto' else self.nonlin
        self.history_ = History()
        self.initialize_callbacks()
        self.initialized_ = True
        return self

    def signal_handler(self):
        self.should_stop_ = False
        self._signal_connector.register()

    def setup_modules(self, **kwargs):
        self.module_, self.optimizer_ = self.setup(self.module_, self.optimizer_, **kwargs)

    def fit(self, X, y=None, X_val=None, X_tst=None, **fit_params):
        """Initialize and fit the module."""
        self.check_stop(note='fit_start')
        if not self.warm_start or not self.initialized_:
            self.initialize()
        # setup model
        self.setup_modules()
        try:
            # start training
            self.notify('on_train_begin', X=X, X_val=X_val, X_tst=X_tst)
            # print model after call on_train_begin
            self.model_summary(**({'verbose': self.verbose} | self.get_params_for('summary')))
            self.fit_loop(X=X, y=y, X_val=X_val, X_tst=X_tst, **fit_params)
        except KeyboardInterrupt:
            pass
        except:
            print(self.note, traceback.format_exc())
        self.notify('on_train_end', X=X, X_val=X_val, X_tst=X_tst)
        self.clean_up()
        return self

    def check_data(self, X, X_val=None, X_tst=None):
        """ check data size and assert same indices on each device. """
        self.print(self.note, f"Data size: trn: {len(X)}, val: {len(X_val or '')}, tst: {len(X_tst or '')}")
        if hasattr(X, 'indices') and self.world_size > 1:
            world_indices = self.all_gather(to_tensor(X.indices, self.device, stack=True)).cpu().tolist()  # world -> local
            for idx in range(1, len(world_indices) - 1):
                diff = pd.Index(world_indices[idx]).difference(pd.Index(world_indices[idx - 1]), sort=False)
                assert len(diff) == 0, self.print(f"Data should be same for every rank, but diff in rank {idx}: {diff}")
        if hasattr(X, 'dataset') and hasattr(X.dataset, 'speakers'):
            self.print(self.note, f"Speakers: trn: {len(set(np.array(X.dataset.speakers)[X.indices]) or '')},",
                       f"val: {len(set(np.array(X_val.dataset.speakers)[X_val.indices]) or '')},", end=' ')
            if not X_tst:
                return
            if isinstance(X_tst, (list, tuple)):
                for i, x in enumerate(X_tst):
                    self.print(f"tst{i}: {len(set(np.array(x.dataset.speakers)[x.indices]))}")
            else:
                self.print(f"tst: {len(set(np.array(X_tst.dataset.speakers)[X_tst.indices]))}")

    def set_epoch(self, dl_trn, epoch):
        if isinstance(dl_trn, dict):
            for _, v in dl_trn.items():
                if hasattr(v, 'sampler') and hasattr(v.sampler, 'set_epoch'):
                    v.sampler.set_epoch(epoch)
        else:
            if hasattr(dl_trn, 'sampler') and hasattr(dl_trn.sampler, 'set_epoch'):
                dl_trn.sampler.set_epoch(epoch)

    def fit_loop(self, X, y=None, X_val=None, X_tst=None, **fit_params):
        """Fit loop for epochs."""
        self.check_stop(note='fit_loop_start')
        if not isinstance(X, (self.dataset, Dataset)):
            X = self.dataset(X, y, **self.get_params_for('dataset'))
        self.check_data(X, X_val, X_tst)
        dl_trn, dl_val, dl_tst = self.get_dataloaders(X, X_val=X_val, X_tst=X_tst)
        # running
        on_epoch_kwargs = {'dataset_train': X, 'dataset_valid': X_val, 'dataset_test': X_tst}
        epoch = 0
        while True:
            self.check_stop(note='epoch_iter_start')
            self.notify('on_epoch_begin', epoch=epoch, **on_epoch_kwargs)
            self.set_epoch(dl_trn, epoch)
            dl_map = {True: dl_trn, False: dl_val, "": dl_tst}  # dl_map after set_epoch (new dl_trn)
            self.grad_warning_ = 0
            for training in [True, False, ""]:
                dl = dl_map[training]
                self.run_single_epoch_list(dl, training=training, epoch=epoch, **fit_params)
            self.notify('on_epoch_end', epoch=epoch, **on_epoch_kwargs)
            # self.clean_up()
            epoch += 1
            if epoch > self.max_epochs:  # self.max_epochs can be changed during training
                break
        return self

    def run_single_epoch_list(self, dataloaders, training, epoch=0, **fit_params):
        if not isinstance(dataloaders, (list, tuple)):
            self.run_single_epoch(dataloaders, training=training, epoch=epoch, **fit_params)
            return
        for i, dl in enumerate(dataloaders):
            idx = i if len(dataloaders) > 1 else None
            self.run_single_epoch(dl, training=training, epoch=epoch, idx=idx, **fit_params)

    def run_single_epoch(self, dataloader, training, epoch=0, idx=None, **fit_params):
        """Compute a single epoch of train or validation."""
        if dataloader is None:
            return
        step_fn = {True: self.train_step, False: self.val_step}[bool(training)]
        idx = "" if idx is None else str(idx)
        prefix = {True: 'trn', False: 'val', "": 'tst'}[training] + idx
        self.set_training(training)
        batch_count, data_len = 0, len(dataloader)
        data_iter = tqdm(enumerate(dataloader), total=data_len, desc=prefix, leave=False,
                         dynamic_ncols=True) if self.local_rank == 0 else enumerate(dataloader)
        for batch_idx, batch in data_iter:
            self.check_stop(note='batch_iter_start')
            self.notify('on_batch_begin', batch=batch, training=training)
            step = step_fn(batch=batch, batch_idx=batch_idx, epoch=epoch, data_len=data_len, training=training, **fit_params)
            # log (pseudo loss and batch_size in ddp)
            step_loss = to_numpy(self._strategy.reduce(step['loss'], reduce_op='mean'))
            step_bsz = to_numpy(self._strategy.reduce(len(step['y_true']), reduce_op='mean'))
            self.history.record_batch(prefix + '_loss', step_loss * self.accumulate_steps)
            self.history.record_batch(prefix + '_batch_size', step_bsz)  # int
            self.notify('on_batch_end', batch=batch, training=training, **to_device(step, 'cpu'))
            batch_count += 1
            if self.local_rank == 0:
                data_iter.set_description(f"{prefix + '_loss'} {step_loss:.3f}", refresh=True)

        self.history.record(prefix + '_batch_count', batch_count)

    def set_training(self, training):
        """Set training state of nn.Module (still autograd but no backprobagation)."""
        self.training = training
        self.module_.train(training is True)
        if hasattr(self.criterion_, 'train'):
            self.criterion_.train(training is True)

    def forward(self, batch, return_outputs=False, **fit_params):
        Xi, yi = self.unpack_data(batch)
        out = self.module_(Xi, **fit_params)
        y_pred = self.predict_proba(logits=out)
        loss = self.criterion_(out, yi)
        outputs = {
            'loss': loss / self.accumulate_steps,
            'y_pred': y_pred,
            'y_true': yi
        }
        return outputs | {'out': out} if return_outputs else outputs

    def infer(self, batch, **fit_params):
        """Inference loss and y_pred."""
        out = {}
        try:
            out = self.forward(batch, **fit_params)
            # assert out['y_true'].shape == out['y_pred'].shape, self.note + \
            #     f" Shapes of y_true {out['y_true'].shape} != y_pred {out['y_pred'].shape}!"
            if out['loss'].isnan().any() or out['loss'].isinf().any():
                if self.grad_warning_ < 2:
                    self.warning(self.note, f"Loss is {out['loss']} at batch {batch}")
                else:
                    self.warning(self.note, f"Loss is {out['loss']}")
                self.grad_warning_ += 1
                # if self.grad_warning_ > 10:
                #     self.should_stop_ = True
        except (RuntimeError, MemoryError):
            self.warning(self.note, traceback.format_exc())
            self.should_stop_ = True
        except:
            print(self.note, traceback.format_exc())
            self.should_stop_ = True
        self.check_stop(note='infer_end')
        return out

    def train_step(self, batch, batch_idx, epoch, data_len, training=True, **fit_params):
        """Inference and optimization."""
        self.set_training(training)
        self.barrier()  # wait for batch
        out = self.infer(batch, **fit_params)
        self.backward(out['loss'])
        if epoch == 0 and batch_idx == 0:
            self.find_unused_params()
            # out['loss'] += 0 * param.mean()
        if ((batch_idx + 1) % self.accumulate_steps == 0) or (batch_idx + 1 >= data_len):
            total_norm = torch.nn.utils.clip_grad_norm_(self.module_.parameters(), self.grad_clip_value)
            if total_norm.isnan() or total_norm.isinf() or total_norm.isclose(torch.zeros_like(total_norm)):
                self.warning(self.note, f"Total grad norm is {total_norm} at batch {batch_idx}!")
                self.grad_warning_ += 1
                # if self.grad_warning_ > 10:
                #     self.should_stop_ = True
            else:
                self.optimizer_.step()
            self.optimizer_.zero_grad()
        self.check_stop(note='train_step_end')
        return out

    # pylint: disable=unused-argument
    def val_step(self, batch, batch_idx=None, epoch=None, data_len=None, training=False, **fit_params):
        """Perform a forward step using batched data and return the resulting loss."""
        self.set_training(training)
        self.barrier()
        with torch.no_grad():
            out = self.infer(batch, **fit_params)
        if self.ddp_test:
            out = self.gather_dict(out)
            out['loss'] = out['loss'].mean(0)
        return out

    def evaluation_step(self, batch, training="", **fit_params):
        """Produce the output used for prediction and scoring."""
        self.check_is_fitted()
        self.set_training(training)
        self.barrier()
        Xi, _ = self.unpack_data(batch)
        with torch.no_grad():
            return self.module_(Xi, **fit_params)

    def forward_iter(self, X, training=False):
        """Yield outputs of module forward calls on each batch of data."""
        dataset = X if isinstance(X, (self.dataset, Dataset)) else self.dataset(X)
        iterator = self.get_iterator(dataset, training=training)
        iterator = tqdm(enumerate(iterator), total=len(iterator), desc='eval', leave=False,
                        dynamic_ncols=True) if self.local_rank == 0 else enumerate(iterator)
        preds = []
        for _, batch in iterator:
            self.check_stop(note='batch_iter_start')
            yp = self.evaluation_step(batch=batch)
            preds.append(to_device(yp, device=self.device))
        return torch.cat(preds, dim=0)

    def predict_proba(self, X=None, logits=None):
        """Return the output of the module's forward method as a numpy."""
        # setup model
        self.setup_modules()
        # predict
        logits = self.forward_iter(X, training=False) if logits is None else logits
        return to_numpy(self.nonlin_(logits))

    def predict(self, X=None, logits=None):
        """Where applicable, return class labels for samples in X."""
        return self.predict_proba(X, logits).argmax(axis=-1)

    def unpack_data(self, batch):
        X, y = batch
        return to_tensor(X, device=self.device), to_tensor(y, device=self.device)

    def get_dataloaders(self, X, X_val=None, X_tst=None):
        # seeding worker_init_fn, DistributedSampler, etc.
        self.print(self.note, f"\033[1;32mbatch_size {self.batch_size_ * self.num_devices * self.accumulate_steps}:",
                   f"{self.batch_size_} * {self.num_devices} device(s) *",
                   f"{self.accumulate_steps} accumulating step(s)\033[0m")
        dl_trn = self.setup_dataloaders(self.get_iterator(X, training=True))
        dl_val = self.get_iterator(X_val, training=False) if X_val else None
        dl_tst = self.get_iterators(X_tst, training="") if X_tst else None
        if self.ddp_test:
            dl_val = self.setup_dataloaders(dl_val) if dl_val else None
            dl_tst = self.setup_dataloaders(dl_tst) if dl_tst else None
        return dl_trn, dl_val, dl_tst

    def get_iterator(self, dataset, training=False, **kwargs):
        """Get an iterator that allows to loop over the batches of the given data."""
        prefix = {True: 'iterator_trn', False: 'iterator_val', "": 'iterator_tst'}[training]
        kwargs = self.get_params_for('iterator') | self.get_params_for(prefix) | kwargs
        if 'batch_size' in kwargs.keys():
            batch_size = kwargs['batch_size']
        else:
            # Using distributed batch_size_ (batch_size // num_devices)
            batch_size = self.batch_size_ if training is True or self.ddp_test else self.batch_size
        kwargs = {'pin_memory': True, 'batch_size': batch_size, 'shuffle': training is True} | kwargs
        if kwargs['batch_size'] == -1:
            kwargs['batch_size'] = len(dataset)
        if 'sampler' in kwargs.keys() and kwargs['sampler'] is not None:
            kwargs['shuffle'] = False
        if training is not True:
            kwargs = kwargs | {'drop_last': False, 'shuffle': False, 'sampler': None, 'batch_sampler': None}
        if 'batch_sampler' in kwargs.keys() and kwargs['batch_sampler'] is not None:
            # batch_sampler is mutually exclusive with batch_size, shuffle, sampler, drop_last
            for k in ['batch_size', 'shuffle', 'sampler', 'drop_last']:
                kwargs.pop(k, None)
        return DataLoader(dataset, **kwargs)

    def get_iterators(self, datasets, training=False):
        if not isinstance(datasets, (list, tuple)):
            return self.get_iterator(datasets, training=training)
        return [self.get_iterator(ds, training=training) for ds in datasets]

    ############################## Environment ##############################
    @property
    def device(self) -> torch.device:
        """The current device this process runs on.
        Use this to create tensors directly on the device if needed.
        """
        return self._strategy.root_device

    @property
    def global_rank(self) -> int:
        """The global index of the current process across all devices and nodes."""
        return getattr(self._strategy, "global_rank", 0)

    @property
    def local_rank(self) -> int:
        """The index of the current process among the processes running on the local node."""
        return getattr(self._strategy, "local_rank", 0)

    @property
    def node_rank(self) -> int:
        """The index of the current node."""
        return getattr(self._strategy, "node_rank", 0)

    @property
    def world_size(self) -> int:
        """The total number of processes running across all devices and nodes."""
        return getattr(self._strategy, "world_size", 1)

    @property
    def is_global_zero(self) -> bool:
        """Wether this rank is rank zero."""
        return self._strategy.is_global_zero

    @property
    def num_nodes(self):
        return getattr(self._strategy, "num_nodes", 1)

    @property
    def device_ids(self):
        """List of device indexes per node."""
        devices = (
            self._strategy.parallel_devices
            if isinstance(self._strategy, ParallelStrategy)
            else [self._strategy.root_device]
        )
        device_ids = []
        for idx, device in enumerate(devices):
            if isinstance(device, torch.device):
                device_ids.append(device.index or idx)
            elif isinstance(device, int):
                device_ids.append(device)
        return device_ids

    @property
    def num_devices(self):
        """Number of devices the trainer uses per node."""
        return len(self.device_ids)

    def print(self, *args, **kwargs):
        """Print something only on the first process."""
        if self.verbose and self.local_rank == 0 and threading.current_thread() is threading.main_thread():
            print(*args, **kwargs)

    def warning(self, *args, **kwargs):
        if self.verbose >= 0 and self.local_rank == 0 and threading.current_thread() is threading.main_thread():
            print(*args, **kwargs)

    def gather_dict(self, data):
        """ Gather a dict of values in world. """
        data = {k: to_tensor(v, device=self.device) for k, v in data.items()}
        enc_keys = []  # encode_str, decode_str for strings
        for k, v in data.items():
            if isinstance(v, (list, tuple)):
                if isinstance(v[0], str):
                    data[k] = torch.LongTensor([encode_str(s) for s in v])
                    enc_keys.append(k)
                else:
                    # TODO: deal with list of tensor
                    data[k] = torch.stack(v, dim=0)
        data = self.all_gather(data)
        data = {k: v.flatten(0, 1) if v.ndim > 1 else v for k, v in data.items()}
        for k in enc_keys:
            data[k] = [decode_str(int(s.cpu().item())) for s in data[k]]
        return data

    def run(self, *args, run_method=None, **kwargs):
        # compatible to LightningLite
        if run_method:
            return getattr(self, run_method)(*args, **kwargs)

    def check_stop(self, save=False, note=''):
        # stop every ddp process if any world process decides to stop
        self.barrier()
        if self._strategy.reduce_boolean_decision(self.should_stop_):
            if note:
                self.warning(f"\033[1;31m♫{self.local_rank}\033[0m Stop at {note}.")
            if save:
                model_file = os.path.join(self.saved_dir, 'last_model.pt')
                optim_file = os.path.join(self.saved_dir, 'last_optim.pt')
                if not os.path.exists(model_file):
                    self.save_params(f_module=model_file, f_optim=optim_file)
            self._signal_connector.teardown()
            raise KeyboardInterrupt

    def teardown(self):
        """Trainer's internal teardown."""
        self._signal_connector.teardown()
        self._strategy.teardown()

    def clean_up(self):
        if torch.cuda.is_available() and torch.cuda.current_device() is not None:
            torch.cuda.empty_cache()
        gc.collect()

    def unwrap(self, model, *optimizers):
        model_ = model._original_module if isinstance(model, _LiteModule) else model
        optimizers_ = [opt.optimizer if isinstance(opt, _LiteOptimizer) else opt for opt in optimizers]
        if optimizers_:
            return [model_] + list(optimizers_)
        return model_

    def wrap(self, state_dict=None, module=None):
        prefix, common_prefix, = '', os.path.commonprefix(list(state_dict.keys()))
        if isinstance(module, _LiteModule):
            prefix = '' if '_original_module.' in common_prefix else '_original_module.'
            module = module._original_module
        if isinstance(module, DDP):
            prefix += '' if 'module.' in common_prefix else 'module.'
        prefix = '' if prefix in common_prefix else prefix
        return {prefix + k: v for k, v in state_dict.items()}

    def setup(self, model, *optimizers, move_to_device: bool = True):
        """Set up a model and **its** optimizers for accelerated training."""
        if not isinstance(model, _LiteModule):
            return super().setup(model, *optimizers, move_to_device=move_to_device)
        if optimizers:
            return [model] + list(optimizers)
        return model

    def model_summary(self, **kwargs):
        if self.local_rank != 0 or not kwargs.get('verbose', True):
            return None
        model = self.unwrap(model=self.module_)
        return summary(model, **({'depth': 2} | (kwargs or {})))

    def find_unused_params(self, reset_grad=None):
        reset_grad = reset_grad if reset_grad is not None else self.reset_grad
        model = self.unwrap(model=self.module_)
        unused_names, unused_params = [], []
        for name, param in model.named_parameters():
            if param.grad is None and param.requires_grad:
                name = name.removesuffix('.weight').removesuffix('.bias')
                if name not in unused_names:
                    unused_names.append(name)
                unused_params.append(param)
                if reset_grad:
                    param.requires_grad = False
        if unused_names and self.verbose:
            self.warning(
                f"\033[1;31m♫{self.local_rank} {unused_names} is unused, make sure all forward pass to loss.\n",
                "You can (1) remove/zeros it, (2) set `lr=0` for its `params_group`, or (3) enable `find_unused_parameters`.\033[0m")
        return unused_params

    ############################## Parmeters (sklearn compatible) ##############################
    def check_is_fitted(self, attributes=None, *args, **kwargs):
        """Checks whether the trainer is initialized."""
        attributes = attributes or ['module_']
        check_is_fitted(self, attributes, *args, **kwargs)

    def initialize_instance(self, instance_or_cls, kwargs):
        valid_kwargs = {}
        for k, v in kwargs.items():
            if k in inspect.getfullargspec(instance_or_cls).args:
                valid_kwargs[k] = v
        return instance_or_cls(**kwargs)

    def instance_or_cls_name(self, instance_or_cls):
        return instance_or_cls.__name__ if isinstance(instance_or_cls, type) else instance_or_cls.__class__.__name__

    def initialize_module(self):
        self.module_ = self.initialize_instance(self.module, self.get_params_for('module'))

    def initialize_criterion(self):
        self.criterion_ = self.initialize_instance(self.criterion, self.get_params_for('criterion'))
        if hasattr(self.criterion_, 'to'):
            self.criterion_ = self.criterion_.to(self.device)

    def initialize_optimizer(self):
        optimizer_args = {'lr': self.lr} | self.get_params_for('optimizer')
        self.lr = optimizer_args['lr']
        params = self.optim_params(self.module_, optimizer_args)
        self.optimizer_ = self.initialize_instance(self.optimizer, {'params': params})

    def optim_params(self, model, optimizer_args, skip_names=['bias', 'norm'], reset_grad=None):
        """Per-parameter options for optimizer."""
        reset_grad = reset_grad if reset_grad is not None else self.reset_grad
        default_keys = inspect.getfullargspec(self.optimizer).args
        for k in list(optimizer_args.keys()):
            if k not in default_keys:
                optimizer_args.pop(k, None)
        params = []
        for name, param in model.named_parameters():
            named_params = {'params': param} | optimizer_args
            if not param.requires_grad:
                if not reset_grad:
                    continue
                # reset grad True, but set lr=0
                param.requires_grad = True
                named_params = named_params | {'lr': 0.}
            if 'weight_decay' in default_keys and (
                    len(param) == 1 or any(sub in name.lower() for sub in skip_names)):
                named_params = named_params | {'weight_decay': 0.}
            params.append(named_params)
        return params

    def _get_param_names(self):
        """ params for BaseEstimator to initialize the trainer, excluded `._*`, `.*_`, or lite methods  """
        to_exclude = ['run', 'fit', 'predict']
        return (k for k in self.__dict__ if not k.startswith('_') and not k.endswith('_') and k not in to_exclude)

    def _get_params_for(self, prefix):
        if not prefix.endswith('__'):
            prefix += '__'
        return {key[len(prefix):]: val for key, val in self.__dict__.items() if key.startswith(prefix)}

    def get_params_for(self, prefix):
        """Collect and return init parameters for an attribute."""
        return self._get_params_for(prefix)

    def _get_params_callbacks(self, deep=True):
        """sklearn's .get_params checks for `hasattr(value,
        'get_params')`. This returns False for a list. But our
        callbacks reside within a list. Hence their parameters have to
        be retrieved separately.
        """
        params = {}
        if not deep:
            return params

        callbacks_ = getattr(self, 'callbacks_', [])
        for key, val in chain(callbacks_, self._default_callbacks):
            name = 'callbacks__' + key
            params[name] = val
            if val is None:  # callback deactivated
                continue
            for subkey, subval in val.get_params().items():
                subname = name + '__' + subkey
                params[subname] = subval
        return params

    def get_params(self, deep=True, to_exclude=[], **kwargs):
        params = BaseEstimator.get_params(self, deep=deep, **kwargs)
        # Callback parameters are not returned by .get_params, needs special treatment.
        params_cb = self._get_params_callbacks(deep=deep)
        params.update(params_cb)
        return {key: val for key, val in params.items() if not match(key, to_exclude)}

    def set_params(self, **kwargs):
        """Set the parameters of this class.
        Valid parameter keys can be listed with ``get_params()``.
        Returns
        -------
        self
        """
        cb_params, special_params, normal_params = {}, {}, {}
        for key, val in kwargs.items():
            if key.startswith('callbacks'):
                cb_params[key] = val
            elif '__' in key or any(key.startswith(prefix) for prefix in self.prefixes_):
                special_params[key] = val
            else:
                normal_params[key] = val

        BaseEstimator.set_params(self, **normal_params)
        for key, val in special_params.items():
            if key.endswith('_'):
                raise ValueError(
                    "Something went wrong here. Please open an issue on "
                    "https://github.com/skorch-dev/skorch/issues detailing what "
                    "caused this error.")
            setattr(self, key, val)

        if cb_params:
            # callbacks need special treatmeant since they are list of tuples
            self.initialize_callbacks()
            self._set_params_callback(**cb_params)
            vars(self).update(cb_params)

        # If the net is not initialized or there are no special params, we can
        # exit as this point, because the special_params have been set as
        # attributes and will be applied by initialize() at a later point in time.
        if not self.initialized_ or not special_params:
            return self
        # if net is initialized, checking kwargs is possible
        # self._validate_params()

        ######################################################
        # Below: Re-initialize parts of the net if necessary #
        ######################################################

        # if there are module params, reinit module, criterion, optimizer
        # if there are criterion params, reinit criterion, optimizer
        # optimizer params don't need to be checked, as they are virtual
        reinit_module = False
        reinit_criterion = False
        reinit_optimizer = False

        component_names = {key.split('__', 1)[0] for key in special_params}
        for prefix in component_names:
            if prefix.startswith('module'):
                reinit_module = True
                reinit_criterion = True
                reinit_optimizer = True
                # if any module is modified, everything needs to be
                # re-initialized, no need to check any further
                break
            if prefix.startswith('criterion'):
                reinit_criterion = True
                reinit_optimizer = True

        if not (reinit_module or reinit_criterion or reinit_optimizer):
            raise ValueError("Something went wrong, please open an issue on "
                             "https://github.com/skorch-dev/skorch/issues")

        if reinit_module:
            self.initialize_module()
        if reinit_criterion:
            self.initialize_criterion()
        if reinit_optimizer:
            self.initialize_optimizer()

        return self

    def _set_params_callback(self, **params):
        """Special handling for setting params on callbacks."""
        # model after sklearn.utils._BaseCompostion._set_params
        # 1. All steps
        if 'callbacks' in params:
            setattr(self, 'callbacks', params.pop('callbacks'))

        # 2. Step replacement
        names, _ = zip(*getattr(self, 'callbacks_'))
        for key in params.copy():
            name = key[11:]  # drop 'callbacks__'
            if '__' not in name and name in names:
                self._replace_callback(name, params.pop(key))

        # 3. Step parameters and other initilisation arguments
        for key in params.copy():
            name = key[11:]
            part0, part1 = name.split('__')
            kwarg = {part1: params.pop(key)}
            callback = dict(self.callbacks_).get(part0)
            if callback is not None:
                callback.set_params(**kwarg)
            else:
                raise ValueError(
                    "Trying to set a parameter for callback {} "
                    "which does not exist.".format(part0))

        return self

    def _replace_callback(self, name, new_val):
        # assumes `name` is a valid callback name
        callbacks_new = self.callbacks_[:]
        for i, (cb_name, _) in enumerate(callbacks_new):
            if cb_name == name:
                callbacks_new[i] = (name, new_val)
                break
        setattr(self, 'callbacks_', callbacks_new)

    def save_params(self, f_module=None, f_optim=None, f_history=None, entire=False):
        """Saves the module's parameters and history."""
        if self.local_rank != 0 and threading.current_thread() != threading.main_thread():
            return
        model, opt = self.unwrap(self.module_, self.optimizer_)
        if f_module is not None:
            # torch.save(self.module_.state_dict(), f_module)
            model.eval()
            self.save(model if entire else model.state_dict(), f_module)
            # model = torch.jit.script(model) # Export to TorchScript
            # model.save(f_module)
        if f_optim is not None:
            self.save(opt if entire else opt.state_dict(), f_optim)
        if f_history is not None:
            self.history.to_file(f_history)
        # save trainer params
        params_to_save = self.get_params(to_exclude=['callbacks*', 'cfg'])
        self.save(params_to_save, os.path.join(self.saved_dir, 'params.pt'))

    def load_params(self, f_module=None, f_optim=None, f_history=None):
        """Loads the the module's parameters and history."""
        # params_dir = os.path.join(self.saved_dir, 'params.pt')
        # if os.path.exists(params_dir):
        #     params = self.load(params_dir)
        #     self.set_params(**params)
        if not f_module and not f_optim and not f_history:
            # if not os.path.exists(params_dir):
            #     self.warning("\033[1;31mNo params to load!\033[0m")
            return
        self.print(self.note + f" Loading params at epoch {len(self.history) - 1}:", end=" ")
        if f_module is not None:
            if isinstance(f_module, (str, Path)) and not os.path.exists(f_module):
                self.warning(f"\033[1;31mModule {f_module} not exists!\033[0m")
            # elif isinstance(obj, RecursiveScriptClass, ScriptModule, ScriptFunction):
            else:
                # state_dict = torch.load(f_module, map_location=self.device)
                state_dict = self.load(f_module) if isinstance(f_module, (str, Path)) else f_module
                if isinstance(state_dict, dict):
                    state_dict = self.wrap(state_dict, self.module_)
                    self.module_.load_state_dict(state_dict, strict=False)
                else:
                    self.module_ = state_dict  # entire
                self.print(f"Module loaded from {f_module if isinstance(f_module, (str, Path)) else 'dict'}")
        if f_optim is not None:
            if isinstance(f_optim, (str, Path)) and not os.path.exists(f_optim):
                self.warning(f"\033[1;31mOptim {f_optim} not exists!\033[0m")
            else:
                state_dict = self.load(f_optim) if isinstance(f_optim, (str, Path)) else f_optim
                if isinstance(state_dict, dict):
                    self.optimizer_.load_state_dict(state_dict)
                else:
                    self.optimizer_ = state_dict  # entire
                self.print(f"Optim loaded from {f_optim if isinstance(f_optim, (str, Path)) else 'dict'}")
        if f_history is not None:
            if not os.path.exists(f_history):
                self.warning(f"\033[1;31mHistory {f_history} not exists!\033[0m")
            else:
                self.history = History.from_file(f_history)
                self.print(f"History loaded from {f_history}")

    def __repr__(self):
        """ Trainer representation: module and initialization state. """
        to_include = ['module']
        to_exclude = []
        parts = [str(self.__class__) + '[uninitialized](']
        if self.initialized_:
            parts = [str(self.__class__) + '[initialized](']
            to_include = ['module_']
            to_exclude = ['module__']

        for key, val in sorted(self.__dict__.items()):
            if not any(key.startswith(prefix) for prefix in to_include):
                continue
            if any(key.startswith(prefix) for prefix in to_exclude):
                continue

            val = str(val)
            if '\n' in val:
                val = '\n  '.join(val.split('\n'))
            parts.append('  {}={},'.format(key, val))

        parts.append(')')
        return '\n'.join(parts)

    ############################## Callback ##############################
    @property
    def _default_callbacks(self):
        return [
            ('trn_loss', PassthroughScoring(name='trn_loss')),
            ('val_loss', PassthroughScoring(name='val_loss')),
            ('tst_loss', PassthroughScoring(name='tst_loss')),
            ('trn_acc', EpochScoring('balanced_accuracy', name='trn_acc', on_train=True)),  # UA
            ('val_acc', EpochScoring('balanced_accuracy', name='val_acc', on_train=False)),
            ('tst_acc', EpochScoring('balanced_accuracy', name='tst_acc', on_train="")),
            ('lr', LearningRate()),
            ('epoch_timer', EpochTimer()),
            ('print_log', PrintLog(floatfmt='.4f', sink=self.print)),
        ]

    @property
    def history(self):
        return self.history_

    @history.setter
    def history(self, value):
        self.history_ = value

    def notify(self, method_name, **cb_kwargs):
        """Call the callback method specified in ``method_name`` with
        parameters specified in ``cb_kwargs``.
        Method names can be one of on_[train|epoch|batch]_[begin|end]
        """
        getattr(self, method_name)(self, **cb_kwargs)
        for _, cb in self.callbacks_:
            cb_name = self.instance_or_cls_name(cb)
            if (len(self.history) - 1) % self.log_interval != 0 and cb_name in ['EpochScoring']:
                continue
            getattr(cb, method_name)(self, **cb_kwargs)

    def on_train_begin(self, net, X=None, y=None, **kwargs):
        pass

    def on_train_end(self, net, X=None, y=None, **kwargs):
        pass

    def on_epoch_begin(self, net, dataset_train=None, dataset_valid=None, dataset_test=None, **kwargs):
        self.history.new_epoch()
        self.history.record('epoch', len(self.history))

    def on_epoch_end(self, net, dataset_train=None, dataset_valid=None, dataset_test=None, **kwargs):
        pass

    def on_batch_begin(self, net, batch=None, training=False, **kwargs):
        self.history.new_batch()

    def on_batch_end(self, net, batch=None, training=False, **kwargs):
        pass

    def initialize_callbacks(self):
        # this init context is for consistency and not being used at the moment
        if self.callbacks == "disable":
            self.callbacks_ = []
            return self
        callbacks_ = []

        class Dummy:
            # We cannot use None as dummy value since None is a
            # legitimate value to be set.
            pass

        for name, cb in self._uniquely_named_callbacks():
            # check if callback itself is changed
            param_callback = getattr(self, 'callbacks__' + name, Dummy)
            if param_callback is not Dummy:  # callback itself was set
                cb = param_callback

            # below: check for callback params
            # don't set a parameter for non-existing callback
            params = self.get_params_for('callbacks__{}'.format(name))
            if (cb is None) and params:
                raise ValueError("Trying to set a parameter for callback {} "
                                 "which does not exist.".format(name))
            if cb is None:
                continue

            if isinstance(cb, type):  # uninitialized:
                cb = cb(**params)
            else:
                cb.set_params(**params)
            cb.initialize()
            callbacks_.append((name, cb))

        # self.callbacks_ = callbacks_
        # specified order, 0~9＜A~Z＜a~z
        names = [
            'PassthroughScoring', 'LearningRate', 'EpochScoring', 'EpochTimer',  # metircs
            'TensorBoard', 'WandbLogger', 'PrintLog',  # loggers
            'EarlyStopping', 'LRScheduler', 'Freezer',  # schedulers
        ]
        orders = {}
        for t in callbacks_:
            k = self.instance_or_cls_name(t[1])
            if k in names:
                orders[k] = chr(ord(str(names.index(k))) - ord('0') + ord('a'))
            else:
                orders[k] = k.lower()

        self.callbacks_ = sorted(callbacks_, key=lambda t: orders[self.instance_or_cls_name(t[1])])

        return self

    def _uniquely_named_callbacks(self):
        """Make sure that the returned dict of named callbacks is unique
        w.r.t. to the callback name. User-defined names will not be
        renamed on conflict, instead an exception will be raised. The
        same goes for the event where renaming leads to a conflict.
        """
        grouped_cbs, names_set_by_user = self._callbacks_grouped_by_name()
        for name, cbs in grouped_cbs.items():
            if len(cbs) > 1 and name in names_set_by_user:
                # override default callbacks if duplicate, using the last one
                cbs = [cbs[-1]]

            for i, cb in enumerate(cbs):
                if len(cbs) > 1:
                    unique_name = '{}_{}'.format(name, i + 1)
                    if unique_name in grouped_cbs:
                        raise ValueError("Assigning new callback name failed "
                                         "since new name '{}' exists already."
                                         .format(unique_name))
                else:
                    unique_name = name
                yield unique_name, cb

    def _callbacks_grouped_by_name(self):
        """Group callbacks by name and collect names set by the user."""
        callbacks, names_set_by_user = OrderedDict(), set()
        for name, cb, named_by_user in self._yield_callbacks():
            if named_by_user:
                names_set_by_user.add(name)
            callbacks[name] = callbacks.get(name, []) + [cb]
        return callbacks, names_set_by_user

    def _yield_callbacks(self):
        """Yield all callbacks set on this instance including
        a set whether its name was set by the user.
        """
        print_logs = []
        for item in self._default_callbacks + (self.callbacks or []):
            if isinstance(item, (tuple, list)):
                named_by_user = True
                name, cb = item
            else:
                named_by_user = False
                cb = item
                name = self.instance_or_cls_name(cb)
            if isinstance(cb, PrintLog) or (cb == PrintLog):
                print_logs.append((name, cb, named_by_user))
            else:
                yield name, cb, named_by_user
        yield from print_logs
